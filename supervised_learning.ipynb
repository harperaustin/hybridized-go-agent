{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "k35Swp-8KoKs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: open-spiel in /usr/local/python/3.12.1/lib/python3.12/site-packages (1.5)\n",
      "Requirement already satisfied: pip>=20.0.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from open-spiel) (24.2)\n",
      "Requirement already satisfied: attrs>=19.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from open-spiel) (24.2.0)\n",
      "Requirement already satisfied: absl-py>=0.10.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from open-spiel) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.21.5 in /home/codespace/.local/lib/python3.12/site-packages (from open-spiel) (2.1.1)\n",
      "Requirement already satisfied: scipy>=1.10.1 in /home/codespace/.local/lib/python3.12/site-packages (from open-spiel) (1.14.1)\n",
      "Requirement already satisfied: ml-collections>=0.1.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from open-spiel) (1.0.0)\n",
      "Requirement already satisfied: six in /home/codespace/.local/lib/python3.12/site-packages (from ml-collections>=0.1.1->open-spiel) (1.16.0)\n",
      "Requirement already satisfied: PyYAML in /home/codespace/.local/lib/python3.12/site-packages (from ml-collections>=0.1.1->open-spiel) (6.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torch in /home/codespace/.local/lib/python3.12/site-packages (2.4.1+cpu)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/codespace/.local/lib/python3.12/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.12/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/codespace/.local/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Needed if running on Colab\n",
    "!pip3 install open-spiel\n",
    "!pip3 install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "O1yAh0sTKs3K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.1)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib confmisc.c:767:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:4732:(_snd_config_evaluate) function snd_func_card_driver returned error: No such file or directory\n",
      "ALSA lib confmisc.c:392:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:4732:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1246:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:4732:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5220:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2642:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "/home/codespace/.local/lib/python3.12/site-packages/torch/__init__.py:955: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:432.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from go_search_problem import GoProblem, GoState\n",
    "from heuristic_go_problems import GoProblemLearnedHeuristic, GoProblemSimpleHeuristic\n",
    "from agents import GreedyAgent, RandomAgent, MCTSAgent, GameAgent\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from game_runner import run_many\n",
    "import pickle\n",
    "\n",
    "torch.set_default_tensor_type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "6XNzHOq6QCQD"
   },
   "outputs": [],
   "source": [
    "def load_dataset(path: str):\n",
    "    with open(path, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    return dataset\n",
    "\n",
    "dataset_5x5 = load_dataset('dataset_5x5.pkl')\n",
    "# dataset_9x9 = load_dataset('9x9_dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "5fAQlSAOLXoj"
   },
   "outputs": [],
   "source": [
    "def save_model(path: str, model):\n",
    "    \"\"\"\n",
    "    Save model to a file\n",
    "    Input:\n",
    "        path: path to save model to\n",
    "        model: Pytorch model to save\n",
    "    \"\"\"\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "    }, path)\n",
    "\n",
    "def load_model(path: str, model):\n",
    "    \"\"\"\n",
    "    Load model from file\n",
    "\n",
    "    Note: you still need to provide a model (with the same architecture as the saved model))\n",
    "\n",
    "    Input:\n",
    "        path: path to load model from\n",
    "        model: Pytorch model to load\n",
    "    Output:\n",
    "        model: Pytorch model loaded from file\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOzaAXYrM4d3"
   },
   "source": [
    "# Task 1: Convert GameState to Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "1hbg6LkAMrZW"
   },
   "outputs": [],
   "source": [
    "def get_features(game_state: GoState):\n",
    "    \"\"\"\n",
    "    Map a game state to a list of features.\n",
    "\n",
    "    Some useful functions from game_state include:\n",
    "        game_state.size: size of the board\n",
    "        get_pieces_coordinates(player_index): get coordinates of all pieces of a player (0 or 1)\n",
    "        get_pieces_array(player_index): get a 2D array of pieces of a player (0 or 1)\n",
    "        \n",
    "        get_board(): get a 2D array of the board with 4 channels (player 0, player 1, empty, and player to move). 4 channels means the array will be of size 4 x n x n\n",
    "    \n",
    "        Descriptions of these methods can be found in the GoState\n",
    "\n",
    "    Input:\n",
    "        game_state: GoState to encode into a fixed size list of features\n",
    "    Output:\n",
    "        features: list of features\n",
    "    \"\"\"\n",
    "    board_size = game_state.size\n",
    "    features = []\n",
    "    # for first 25 features, use just a 1 or 0 to indicate if a black piece is in the slot\n",
    "    black_player_pieces = game_state.get_pieces_array(0)\n",
    "    black_total_pieces = sum(sum(black_player_pieces))\n",
    "    for row in black_player_pieces:\n",
    "        for piece in row:\n",
    "            features.append(piece)\n",
    "    # for second 25 features, use just a 1 or 0 to indicate if a white piece is in the slot\n",
    "    white_player_pieces = game_state.get_pieces_array(1)\n",
    "    white_total_pieces = sum(sum(white_player_pieces))\n",
    "    for row in white_player_pieces:\n",
    "        for piece in row:\n",
    "            features.append(piece)\n",
    "            \n",
    "    # add total number of pieces\n",
    "    features.append(black_total_pieces)\n",
    "    features.append(white_total_pieces)\n",
    "    # finally append the player to move\n",
    "    features.append(game_state.player_to_move())\n",
    "\n",
    "\n",
    "    # the solution might just be calling getboard() and flattening, look into this:\n",
    "\n",
    "    \n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "2cpr86wH8W3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoState(komi=0.5, to_play=B, history.size()=0)\n",
      "\n",
      " 5 +++++\n",
      " 4 +++++\n",
      " 3 +++++\n",
      " 2 +++++\n",
      " 1 +++++\n",
      "   ABCDE\n",
      "\n",
      "features [np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), 0]\n",
      "Action # 11\n",
      "Game Result -1.0\n"
     ]
    }
   ],
   "source": [
    "# Print information about first data point\n",
    "data_point = dataset_5x5[0]\n",
    "features = get_features(data_point[0])\n",
    "action = data_point[1]\n",
    "result = data_point[2]\n",
    "print(data_point[0])\n",
    "print(\"features\", features)\n",
    "print(\"Action #\", action)\n",
    "print(\"Game Result\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YI86jYgcOfHC"
   },
   "source": [
    "# Task 2: Supervised Learning of a Value Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "boPRx0o5Bqq9"
   },
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "      super(ValueNetwork, self).__init__()\n",
    "\n",
    "      # output size should be 1, as we are predicting a value in between [-1,1]\n",
    "      output_size = 1\n",
    "\n",
    "      # add more layers\n",
    "      self.layer1 = nn.Linear(input_size, 64)\n",
    "      self.layer2 = nn.Linear(64, 32)\n",
    "      self.layer3 = nn.Linear(32, 10)\n",
    "      self.layer4 = nn.Linear(10, output_size)\n",
    "      \n",
    "      self.tanh = nn.Tanh()\n",
    "      self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "      \"\"\"\n",
    "      Run forward pass of network\n",
    "\n",
    "      Input:\n",
    "        x: input to network\n",
    "      Output:\n",
    "        output of network\n",
    "      \"\"\"\n",
    "      # relu, tanh, relu, sigmoid\n",
    "      z1 = self.layer1(x)\n",
    "      a1 = torch.relu(z1)\n",
    "      z2 = self.layer2(a1)\n",
    "      a2 = self.sigmoid(z2)\n",
    "      z3 = self.layer3(a2)\n",
    "      a3 = torch.relu(z3)\n",
    "      z4 = self.layer4(a3)\n",
    "      output = self.sigmoid(z4)\n",
    "      return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "83a6vGLqB4E7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted Value tensor([0.4920], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# This will not produce meaningful outputs until trained, but you can test for syntax errors\n",
    "features_tensor = torch.Tensor(features)\n",
    "value_net = ValueNetwork(len(features))\n",
    "print(\"predicted Value\", value_net(features_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Rq8CokTvOyrI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "def train_value_network(dataset, num_epochs, learning_rate):\n",
    "    \"\"\"\n",
    "    Train a value network on the provided dataset.\n",
    "\n",
    "    Input:\n",
    "        dataset: list of (state, action, result) tuples\n",
    "        num_epochs: number of epochs to train for\n",
    "        learning_rate: learning rate for gradient descent\n",
    "    Output:\n",
    "        model: trained model\n",
    "    \"\"\"\n",
    "    # Make sure dataset is shuffled for better performance\n",
    "    random.shuffle(dataset)\n",
    "    # You may find it useful to create train/test sets to better track performance/overfit/underfit\n",
    "    # calculate these just to get the correct input size for the value network.\n",
    "    state = dataset[0][0]\n",
    "    features = get_features(state)\n",
    "    features_tensor = torch.Tensor(features)\n",
    "\n",
    "    model = ValueNetwork(len(features_tensor))\n",
    "    model = model.float()\n",
    "    # NEED TO SPLIT DATASET INTO TRAINING AND TESTING???\n",
    "\n",
    "    # Use MSE as loss function\n",
    "    loss_function = nn.MSELoss()\n",
    "\n",
    "    # You can use Adam, which is stochastic gradient descent with ADAptive Momentum\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    batch_size = 32\n",
    "    batch_loss = 0.0\n",
    "    batch_counter = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        for data_point in dataset:\n",
    "            state = data_point[0]\n",
    "            features = get_features(state)\n",
    "            features_tensor = torch.tensor(features, dtype=torch.float32)\n",
    "\n",
    "            # Note: You will have to convert the label to a torch tensor to use with torch's loss functions\n",
    "            # make the label be the result of the game from this data point\n",
    "            label = torch.tensor(data_point[2], dtype=torch.float32)\n",
    "\n",
    "            # make a prediction with the given features tensor\n",
    "            prediction = model(features_tensor)\n",
    "            # compute loss\n",
    "            loss = loss_function(prediction, label)\n",
    "            batch_loss += loss\n",
    "            batch_counter += 1\n",
    "            if batch_counter % batch_size == 0:\n",
    "                # Call backward to run backward pass and compute gradients\n",
    "                batch_loss.backward()\n",
    "\n",
    "                # Run gradient descent step with optimizer\n",
    "                optimizer.step()\n",
    "\n",
    "                # Reset gradient for next batch\n",
    "                optimizer.zero_grad()\n",
    "                batch_loss = 0.0\n",
    "\n",
    "    return model\n",
    "\n",
    "value_model = train_value_network(dataset_5x5, 10, 1e-4)\n",
    "save_model(\"value_model.pt\", value_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekP8mzDaTOUM"
   },
   "source": [
    "## Comparing Learned Value function against other Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "UWl3dLOnTbiD"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ValueNetwork' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 66\u001b[0m\n\u001b[1;32m     62\u001b[0m     learned_agent \u001b[38;5;241m=\u001b[39m GreedyAgent(heuristic_search_problem)\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m learned_agent\n\u001b[0;32m---> 66\u001b[0m learned_agent \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_value_agent_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m agent2 \u001b[38;5;241m=\u001b[39m GreedyAgent(GoProblemSimpleHeuristic)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGreedy Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m, agent2)\n",
      "Cell \u001b[0;32mIn[23], line 58\u001b[0m, in \u001b[0;36mcreate_value_agent_from_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue_model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m feature_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m53\u001b[39m\n\u001b[0;32m---> 58\u001b[0m model \u001b[38;5;241m=\u001b[39m load_model(model_path, \u001b[43mValueNetwork\u001b[49m(feature_size))\n\u001b[1;32m     59\u001b[0m heuristic_search_problem \u001b[38;5;241m=\u001b[39m GoProblemLearnedHeuristic(model)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# TODO: Try with other heuristic agents (IDS/AB/Minimax)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ValueNetwork' is not defined"
     ]
    }
   ],
   "source": [
    "class GoProblemLearnedHeuristic(GoProblem):\n",
    "    def __init__(self, model=None, state=None):\n",
    "        super().__init__(state=state)\n",
    "        self.model = model\n",
    "        \n",
    "    def __call__(self, model=None):\n",
    "        \"\"\"\n",
    "        Use the model to compute a heuristic value for a given state.\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def encoding(self, state):\n",
    "        \"\"\"\n",
    "        Get encoding of state (convert state to features)\n",
    "        Note, this may call get_features() from Task 1. \n",
    "\n",
    "        Input:\n",
    "            state: GoState to encode into a fixed size list of features\n",
    "        Output:\n",
    "            features: list of features\n",
    "        \"\"\"\n",
    "        return get_features(state)\n",
    "\n",
    "    def heuristic(self, state, player_index):\n",
    "        \"\"\"\n",
    "        Return heuristic (value) of current state\n",
    "\n",
    "        Input:\n",
    "            state: GoState to encode into a fixed size list of features\n",
    "            player_index: index of player to evaluate heuristic for\n",
    "        Output:\n",
    "            value: heuristic (value) of current state\n",
    "        \"\"\"\n",
    "        \n",
    "        value = 0\n",
    "        # get encoding for the state:\n",
    "        state_encoding = self.encoding(state)\n",
    "        features_tensor = torch.Tensor(state_encoding)\n",
    "        value = self.model(features_tensor)\n",
    "        # create heuristic value based on this state\n",
    "        # use return value you get from value Network\n",
    "        \n",
    "        # Note, your agent may perform better if you force it not to pass\n",
    "        # (i.e., don't select action #25 on a 5x5 board unless necessary)\n",
    "        return value\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"Learned Heuristic\"\n",
    "\n",
    "\n",
    "def create_value_agent_from_model():\n",
    "    \"\"\"\n",
    "    Create agent object from saved model. This (or other methods like this) will be how your agents will be created in gradescope and in the final tournament.\n",
    "    \"\"\"\n",
    "\n",
    "    model_path = \"value_model.pt\"\n",
    "    feature_size = 53\n",
    "    model = load_model(model_path, ValueNetwork(feature_size))\n",
    "    heuristic_search_problem = GoProblemLearnedHeuristic(model)\n",
    "\n",
    "    # TODO: Try with other heuristic agents (IDS/AB/Minimax)\n",
    "    learned_agent = GreedyAgent(heuristic_search_problem)\n",
    "\n",
    "    return learned_agent\n",
    "\n",
    "learned_agent = create_value_agent_from_model()\n",
    "agent2 = GreedyAgent(GoProblemSimpleHeuristic)\n",
    "print(\"Greedy Agent\", agent2)\n",
    "print(\"Learned Agent\", learned_agent)\n",
    "\n",
    "run_many(learned_agent, GreedyAgent(), 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUUOOIYhRjoT"
   },
   "source": [
    "# Task 3: Supervised Learning of a Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "dHgeNqBeBm3b"
   },
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, board_size=5):\n",
    "      super(PolicyNetwork, self).__init__()\n",
    "\n",
    "      output_size = (board_size * board_size) + 1\n",
    "\n",
    "      self.layer1 = nn.Linear(input_size, 64)\n",
    "      self.layer2 = nn.Linear(64, 32)\n",
    "      self.layer3 = nn.Linear(32, 64)\n",
    "      self.layer4 = nn.Linear(64, output_size)\n",
    "      self.tanh = nn.Tanh()\n",
    "      self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "      z1 = self.layer1(x)\n",
    "      a1 = self.tanh(z1)\n",
    "      z2 = self.layer2(a1)\n",
    "      a2 = self.tanh(z2)\n",
    "      z3 = self.layer3(a2)\n",
    "      a3 = torch.relu(z3)\n",
    "      z4 = self.layer4(a3)\n",
    "      output = self.sigmoid(z4)\n",
    "      return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "toR5q6qrBvUI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Action Probabilities tensor([0.5144, 0.4915, 0.5075, 0.5195, 0.4720, 0.4960, 0.5170, 0.5047, 0.4855,\n",
      "        0.5174, 0.5087, 0.5141, 0.4923, 0.4894, 0.4997, 0.5040, 0.5171, 0.5209,\n",
      "        0.4849, 0.5056, 0.5024, 0.4911, 0.5303, 0.4885, 0.4793, 0.4944],\n",
      "       grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# This will not produce meaningful outputs until trained, but you can test for syntax errors\n",
    "features_tensor = torch.Tensor(features)\n",
    "policy_net = PolicyNetwork(len(features))\n",
    "print(\"Predicted Action Probabilities\", policy_net(features_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "G6-P_g6wRi-Y"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_5x5' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 66\u001b[0m\n\u001b[1;32m     62\u001b[0m                 optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m---> 66\u001b[0m policy_net \u001b[38;5;241m=\u001b[39m train_policy_network(\u001b[43mdataset_5x5\u001b[49m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m1e-4\u001b[39m)\n\u001b[1;32m     67\u001b[0m save_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, policy_net)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset_5x5' is not defined"
     ]
    }
   ],
   "source": [
    "def train_policy_network(dataset, num_epochs, learning_rate):\n",
    "\n",
    "    \"\"\"\n",
    "    Train a policy network on the provided dataset.\n",
    "\n",
    "    Input:\n",
    "        dataset: list of (state, action, result) tuples\n",
    "        num_epochs: number of epochs to train for\n",
    "        learning_rate: learning rate for gradient descent\n",
    "    Output:\n",
    "        model: trained model\n",
    "    \"\"\"\n",
    "    random.shuffle(dataset)\n",
    "    # needed to get the input size\n",
    "    state = dataset[0][0]\n",
    "    features = get_features(state)\n",
    "    features_tensor = torch.Tensor(features)\n",
    "    \n",
    "    # create model\n",
    "    model = PolicyNetwork(len(features_tensor))\n",
    "    model = model.float()\n",
    "\n",
    "    # TODO: Specify Loss Function\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # You can use Adam, which is stochastic gradient descent with ADAptive Momentum\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    batch_size = 32\n",
    "    batch_loss = 0\n",
    "    batch_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for data_point in dataset:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # TODO: Get features from state and convert features to torch tensor\n",
    "            state = data_point[0]\n",
    "            features = get_features(state)\n",
    "            features_tensor = torch.tensor(features, dtype=torch.float32)\n",
    "\n",
    "            # TODO: What should the desired output of the value network be?\n",
    "            # Note: You will have to convert the label to a torch tensor to use with torch's loss functions\n",
    "            label = torch.tensor(data_point[1], dtype=torch.long)\n",
    "\n",
    "            # TODO: Get model estimate of value\n",
    "            prediction = model(features_tensor)\n",
    "\n",
    "            # TODO: Compute Loss for data point\n",
    "            loss = loss_function(prediction, label)\n",
    "            batch_loss += loss\n",
    "            batch_counter += 1\n",
    "            if batch_counter % batch_size == 0:\n",
    "\n",
    "\n",
    "                # Call backward to run backward pass and compute gradients\n",
    "                loss.backward()\n",
    "\n",
    "                # Run gradient descent step with optimizer\n",
    "                optimizer.step()\n",
    "\n",
    "                # Reset gradient\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "    return model\n",
    "\n",
    "policy_net = train_policy_network(dataset_5x5, 10, 1e-4)\n",
    "save_model(\"policy_model.pt\", policy_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uU3ZxNNi-gWc"
   },
   "source": [
    "## Comparing Learned Policy against other Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "UNqXigG6-dHG"
   },
   "outputs": [],
   "source": [
    "class PolicyAgent(GameAgent):\n",
    "    def __init__(self, search_problem, model_path, board_size=5):\n",
    "        super().__init__()\n",
    "        self.search_problem = search_problem\n",
    "        self.model = load_model(model_path, PolicyNetwork(51, 5))\n",
    "        self.board_size = board_size\n",
    "\n",
    "    def encoding(self, state):\n",
    "        # get features from the state\n",
    "        return get_features(state)\n",
    "\n",
    "  # action = random.choice(self.search_problem.get_available_actions(game_state))\n",
    "    def get_move(self, game_state, time_limit=1):\n",
    "      \"\"\"\n",
    "      Get best action for current state using self.model\n",
    "\n",
    "      Input:\n",
    "        game_state: current state of the game\n",
    "        time_limit: time limit for search (This won't be used in this agent)\n",
    "      Output:\n",
    "        action: best action to take\n",
    "      \"\"\"\n",
    "\n",
    "      # TODO: Select LEGAL Best Action predicted by model\n",
    "      # The top prediction of your model may not be a legal move!\n",
    "\n",
    "      # get a tensor of the features from the current game state\n",
    "      features_tensor = torch.tensor(self.encoding(game_state), dtype=torch.float32)\n",
    "      model_output = self.model(features_tensor)\n",
    "      legal_actions = self.search_problem.get_available_actions(game_state)\n",
    "      #print(legal_actions)\n",
    "      for i in range(len(model_output)):\n",
    "          if i not in legal_actions:\n",
    "              model_output[i] = float('-inf')\n",
    "      # get best legal action\n",
    "      action = torch.argmax(model_output).item()\n",
    "      # if the selected action is to pass, but there are other options, choose other options\n",
    "      if action == 25 and len(legal_actions) > 1:\n",
    "          legal_actions.remove(25)\n",
    "          action = legal_actions[torch.argmax(model_output[legal_actions])]\n",
    "      #print(action)\n",
    "      # Note, you may want to force your policy not to pass their turn unless necessary\n",
    "      assert action in self.search_problem.get_available_actions(game_state)\n",
    "      \n",
    "      return action\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"Policy Agent\"\n",
    "    \n",
    "def create_policy_agent_from_model():\n",
    "    \"\"\"\n",
    "    Create agent object from saved model. This (or other methods like this) will be how your agents will be created in gradescope and in the final tournament.    \n",
    "    \"\"\"\n",
    "\n",
    "    model_path = \"policy_model.pt\"\n",
    "    agent = PolicyAgent(GoProblem(size=5), model_path)\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "8j6tGngt_LVu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3012/2588542451.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Agent Policy Agent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 20.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 1: Policy Agent Score: 0.0\n",
      "Agent 2: GreedyAgent + Simple Heuristic Score: 0.0\n",
      "Agent 1: Policy Agent Score with Black (first move): -20.0\n",
      "Agent 2: GreedyAgent + Simple Heuristic Score with Black (first move): -20.0\n",
      "Agent 1: Policy Agent Average Duration: 0.00037591576576232914\n",
      "Agent 2: GreedyAgent + Simple Heuristic Average Duration: 0.0005226712226867674\n",
      "Agent 1: Policy Agent Longest Duration: 0.003470897674560547\n",
      "Agent 2: GreedyAgent + Simple Heuristic Longest Duration: 0.006167411804199219\n",
      "Agent 1: Policy Agent Average Time Remaining: 39.99060210585594\n",
      "Agent 2: GreedyAgent + Simple Heuristic Average Time Remaining: 39.98693321943283\n",
      "Agent 1: Policy Agent Min Time Remaining: 39.982686042785645\n",
      "Agent 2: GreedyAgent + Simple Heuristic Min Time Remaining: 39.972185134887695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 0.0)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# policy_agent = PolicyAgent(GoProblem(size=5), \"policy_model.pt\")\n",
    "policy_agent = create_policy_agent_from_model()\n",
    "print(\"Policy Agent\", policy_agent)\n",
    "run_many(policy_agent, GreedyAgent(), 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9z2azdVNBxYS"
   },
   "source": [
    "# Submitting\n",
    "\n",
    "After you've completed all the tasks in this notebook, you'll want to add your agents to your agents.py file. You'll want to copy the necessary function and class definitions for PolicyAgent, GoProblemLearnedHeuristic, PolicyNetwork, ValueNetwork, and any other methods you referenced. Your agents will ultimately be tested on gradescope by calling create_value_agent_from_model or by create_policy_agent_from_model."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
